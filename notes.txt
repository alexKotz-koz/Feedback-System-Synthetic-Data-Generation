https://platform.openai.com/docs/api-reference/chat

Temperature (temperature):
    - number or null
    - Optional
    - Defaults to 1
    - What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    - We generally recommend altering this or top_p but not both.

Top_p (top_p): 
    - number or null
    - Optional
    - Defaults to 1
    - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    - We generally recommend altering this or temperature but not both.